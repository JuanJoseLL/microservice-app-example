trigger:
  branches:
    include:
      - dev
      - master # Deploy main to prod, dev to dev environment
  paths:
    include: # Trigger only if app code or k8s manifests change
      - 'auth-api/'
      - 'users-api/'
      - 'todos-api/'
      - 'log-message-processor/'
      - 'frontend/'
      - 'k8s/'
      - 'azure-pipelines-app.yml'
    exclude:
      - '**/*.md' # Exclude markdown changes

pool:
  vmImage: ubuntu-latest

variables:
  # Determine environment based on branch
  - name: environment
    ${{ if eq(variables['Build.SourceBranchName'], 'master') }}:
      value: 'prod'
    ${{ if eq(variables['Build.SourceBranchName'], 'dev') }}:
      value: 'dev'
  # Link to the variable group containing infra outputs and secrets
  - group: app-variables-${{ variables.environment }} # e.g., app-variables-dev or app-variables-prod

  # Define image names consistently
  - name: imageAuthApi
    value: 'auth-api'
  - name: imageUsersApi
    value: 'users-api'
  - name: imageTodosApi
    value: 'todos-api'
  - name: imageLogProcessor
    value: 'log-message-processor'
  - name: imageFrontend
    value: 'frontend'

  # Kubernetes details
  - name: k8sNamespace
    value: 'apps-$(environment)'
  # - name: k8sImagePullSecretName # No longer needed if relying on AKS-ACR integration
  #   value: 'acr-secret'
  - name: k8sRedisSecretName
    value: 'redis-secret'
  - name: k8sAppSecretName
    value: 'app-secrets'
  - name: k8sManifestPath
    value: 'k8s'

  # Service Connection Names
  - name: acrServiceConnection
    value: 'ACR-Service-Connection' # Replace with your ACR Service Connection name
  - name: aksServiceConnection
    value: 'aks-prod-microAppProd-prod-pheasant-aks-apps-prod-1745099156012' # Replace with your AKS Service Connection name

stages:
  # =========================================================================
  # Stage 1: Build and Push ONLY CHANGED Docker Images
  # =========================================================================
  - stage: BuildAndPush
    displayName: Build & Push Changed Images
    jobs:
      - job: BuildPushMatrix
        displayName: Build and Push Services
        strategy:
          matrix:
            AuthApi:
              imageName: $(imageAuthApi)
              buildContext: 'auth-api'
            UsersApi:
              imageName: $(imageUsersApi)
              buildContext: 'users-api'
            TodosApi:
              imageName: $(imageTodosApi)
              buildContext: 'todos-api'
            LogProcessor:
              imageName: $(imageLogProcessor)
              buildContext: 'log-message-processor'
            Frontend:
              imageName: $(imageFrontend)
              buildContext: 'frontend'
        steps:
          # Step 1: Checkout with full history to allow diffing
          - checkout: self
            fetchDepth: 0 # Fetch all history for accurate diff
            displayName: Checkout Full History

          # Step 2: Detect changes and set output variable
          - task: Bash@3
            name: DetectChanges # Give the step a name to reference its output variable
            displayName: 'Detect Changes in $(buildContext)'
            inputs:
              targetType: 'inline'
              script: |
                set -e
                SHOULD_BUILD="false" # Default to not building
                echo "Checking for changes in $(buildContext)/ between HEAD and HEAD~1"
                # Use git diff --quiet. Exits with 1 if there are changes.
                # Compare against the previous commit on the current branch
                if ! git diff --quiet HEAD~1 HEAD -- "$(Build.SourcesDirectory)/$(buildContext)/"; then
                  echo "Changes detected in $(buildContext). Setting SHOULD_BUILD=true."
                  SHOULD_BUILD="true"
                else
                  echo "No changes detected in $(buildContext)."
                fi
                # Set an output variable for this job instance
                echo "##vso[task.setvariable variable=ShouldBuild;isOutput=true]$SHOULD_BUILD"

          # Step 3: Build and Push ONLY if changes were detected
          - task: Docker@2
            displayName: 'Build and Push $(imageName) (if changed)'
            # Condition: Only run if the ShouldBuild variable was set to true in the previous step
            # Note: Accessing variable from the same job uses variables['STEP_NAME.VARIABLE_NAME']
            condition: eq(variables['DetectChanges.ShouldBuild'], 'true')
            inputs:
              containerRegistry: '$(acrServiceConnection)'
              repository: '$(imageName)'
              command: 'buildAndPush'
              Dockerfile: '$(Build.SourcesDirectory)/$(buildContext)/Dockerfile'
              buildContext: '$(Build.SourcesDirectory)/$(buildContext)'
              tags: '$(Build.BuildId)' # Tag with the unique build ID
              addPipelineData: false

  # =========================================================================
  # Stage 2: Deploy to AKS
  # =========================================================================
  - stage: DeployToAKS
    displayName: Deploy Applications to AKS
    dependsOn: BuildAndPush
    # Condition: Run if Build stage succeeded/skipped AND branch is master/dev
    condition: and(in(dependencies.BuildAndPush.result, 'Succeeded', 'SucceededWithIssues', 'Skipped'), or(eq(variables['Build.SourceBranchName'], 'master'), eq(variables['Build.SourceBranchName'], 'dev')))
    jobs:
      - deployment: DeployApps
        displayName: Deploy to AKS ($(environment))
        environment: 'aks-$(environment)'
        strategy:
          runOnce:
            deploy:
              steps:
                # STEP 0: Checkout
                - checkout: self
                  fetchDepth: 0 # Needed for change detection script below
                  displayName: Checkout Repository

                # STEP 1: Detect K8s/Pipeline Changes (to force deployment)
                - task: Bash@3
                  name: DetectInfraChanges # Step name to reference output variable
                  displayName: 'Detect Changes in k8s/ or Pipeline YAML'
                  inputs:
                    targetType: 'inline'
                    script: |
                      set -e
                      FORCE_DEPLOY="false"
                      echo "Checking for changes in k8s/ or azure-pipelines-app.yml between HEAD~1 and HEAD"
                      # Compare against the previous commit on the current branch
                      if ! git diff --quiet HEAD~1 HEAD -- "$(Build.SourcesDirectory)/k8s/" "$(Build.SourcesDirectory)/azure-pipelines-app.yml"; then
                        echo "Changes detected in k8s/ or pipeline YAML. Forcing deployment of all services."
                        FORCE_DEPLOY="true"
                      else
                        echo "No infrastructure changes detected."
                      fi
                      # Set output variable for this job
                      echo "##vso[task.setvariable variable=ForceDeploy;isOutput=true]$FORCE_DEPLOY"

                # STEP 1.5: Debug K8s Directory Check
                - task: Bash@3
                  displayName: 'Debug: Check for K8s Manifests Directory'
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "Pipeline Workspace: $(Pipeline.Workspace)"
                      echo "Build Sources Directory: $(Build.SourcesDirectory)"
                      echo "Listing contents of $(Build.SourcesDirectory):"
                      ls -la $(Build.SourcesDirectory)
                      echo "Checking for $(Build.SourcesDirectory)/$(k8sManifestPath):"
                      if [ -d "$(Build.SourcesDirectory)/$(k8sManifestPath)" ]; then
                        echo "Directory $(Build.SourcesDirectory)/$(k8sManifestPath) found. Contents:"
                        ls -la "$(Build.SourcesDirectory)/$(k8sManifestPath)"
                      else
                        echo "ERROR: Directory $(Build.SourcesDirectory)/$(k8sManifestPath) NOT FOUND."
                        exit 1
                      fi

                # STEP 2: Replace Tokens in Manifests
                - task: replacetokens@5
                  displayName: 'Replace Tokens in K8s Manifests'
                  inputs:
                    rootDirectory: '$(Build.SourcesDirectory)/$(k8sManifestPath)'
                    targetFiles: '**/*.yaml'
                    encoding: 'auto'
                    writeBOM: false
                    actionOnMissing: 'warn'
                    keepToken: false
                    tokenPrefix: '#{'
                    tokenSuffix: '}#'
                    useLegacyPattern: false
                    variables: |
                      k8sNamespace = $(k8sNamespace)
                      acrName = $(acrName)
                      # Replace Build.BuildId in all files. Apply task condition controls actual deployment.
                      Build.BuildId = $(Build.BuildId)
                      # k8sImagePullSecretName = $(k8sImagePullSecretName) # Removed as we rely on AKS-ACR integration

                # STEP 3: Ensure Namespace exists
                - task: KubernetesManifest@0
                  displayName: Ensure Namespace $(k8sNamespace) exists
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    manifests: '$(Build.SourcesDirectory)/$(k8sManifestPath)/namespace.yaml'

                # STEP 4: Create/Update Secrets (Excluding acr-secret)
                - task: KubernetesManifest@0
                  displayName: Create/Update Redis Secret ($(k8sRedisSecretName))
                  inputs:
                    action: 'createSecret'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    secretType: 'generic'
                    secretName: $(k8sRedisSecretName)
                    secretArguments: '--from-literal=redisHost=$(redisHostName) --from-literal=redisKey=$(redisPrimaryKey)'
                    force: true

                - task: KubernetesManifest@0
                  displayName: Create/Update App Secrets (JWT) ($(k8sAppSecretName))
                  inputs:
                    action: 'createSecret'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    secretType: 'generic'
                    secretName: $(k8sAppSecretName)
                    secretArguments: '--from-literal=jwtSecret=$(jwtSecretValue)'
                    force: true

                # STEP 5: Apply App ConfigMap (Generally always apply)
                - task: KubernetesManifest@0
                  displayName: Apply App ConfigMap
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: '$(Build.SourcesDirectory)/$(k8sManifestPath)/app-configmap.yaml'

                # --- STEP 6: Conditional Service Deployments ---
                # Accessing output variables from a previous stage: dependencies.<StageName>.outputs['<JobName>.<StepName>.<VariableName>']
                # Note: <JobName> includes the matrix key, e.g., BuildPushMatrix_AuthApi

                - task: KubernetesManifest@0
                  displayName: Apply Auth API Manifests (if changed or forced)
                  condition: or(eq(variables['DetectInfraChanges.ForceDeploy'], 'true'), eq(dependencies.BuildAndPush.outputs['BuildPushMatrix_AuthApi.DetectChanges.ShouldBuild'], 'true'))
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: |
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/auth-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/auth-api-service.yaml

                - task: KubernetesManifest@0
                  displayName: Apply Users API Manifests (if changed or forced)
                  condition: or(eq(variables['DetectInfraChanges.ForceDeploy'], 'true'), eq(dependencies.BuildAndPush.outputs['BuildPushMatrix_UsersApi.DetectChanges.ShouldBuild'], 'true'))
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: |
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/users-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/users-api-service.yaml

                - task: KubernetesManifest@0
                  displayName: Apply Todos API Manifests (if changed or forced)
                  condition: or(eq(variables['DetectInfraChanges.ForceDeploy'], 'true'), eq(dependencies.BuildAndPush.outputs['BuildPushMatrix_TodosApi.DetectChanges.ShouldBuild'], 'true'))
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: |
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/todos-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/todos-api-service.yaml

                - task: KubernetesManifest@0
                  displayName: Apply Log Processor Manifests (if changed or forced)
                  condition: or(eq(variables['DetectInfraChanges.ForceDeploy'], 'true'), eq(dependencies.BuildAndPush.outputs['BuildPushMatrix_LogProcessor.DetectChanges.ShouldBuild'], 'true'))
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: |
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/log-processor-deployment.yaml

                - task: KubernetesManifest@0
                  displayName: Apply Frontend Manifests (if changed or forced)
                  condition: or(eq(variables['DetectInfraChanges.ForceDeploy'], 'true'), eq(dependencies.BuildAndPush.outputs['BuildPushMatrix_Frontend.DetectChanges.ShouldBuild'], 'true'))
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: |
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/frontend-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/frontend-service.yaml

                # STEP 7: Verify Deployments & Output Endpoints
                - task: Bash@3
                  displayName: Verify Deployments & Output Endpoints
                  condition: always() # Run even if deployment tasks were skipped/failed
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "Waiting up to 5 minutes for deployments in namespace $(k8sNamespace) to stabilize..."
                      # Wait for all deployments in the target namespace to become available
                      # This will wait for deployments that were actually updated or already stable
                      if ! kubectl wait --namespace $(k8sNamespace) --for=condition=available deployment --all --timeout=5m; then
                        echo "WARNING: Not all deployments became available/stable within the timeout."
                      else
                         echo "All relevant deployments in $(k8sNamespace) appear available/stable."
                      fi

                      echo ""
                      echo "=================================================="
                      echo "Current Deployment Status"
                      echo "Namespace: $(k8sNamespace)"
                      echo "=================================================="
                      # ... (rest of verification script: get deployment, pods, svc, check endpoints) ...
                      echo ""
                      echo "----- Deployment Status -----"
                      kubectl get deployment -n $(k8sNamespace)
                      echo ""
                      echo "----- Pod Status -----"
                      kubectl get pods -n $(k8sNamespace) -o wide
                      echo ""
                      echo "----- Services -----"
                      kubectl get svc -n $(k8sNamespace) -o wide
                      echo ""
                      echo "=================================================="
                      echo " Checking Service Endpoints..."
                      echo "=================================================="
                      echo ""

                      # Check Frontend LoadBalancer IP
                      echo "Checking Frontend LoadBalancer IP (Service: frontend-service)..."
                      FRONTEND_IP=""
                      RETRY_COUNT=0
                      MAX_RETRIES=6 # Wait up to 3 minutes (6 * 30s)
                      while [ -z "$FRONTEND_IP" ] && [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                        FRONTEND_IP=$(kubectl get svc frontend-service -n $(k8sNamespace) -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
                        if [ -z "$FRONTEND_IP" ]; then
                          echo "Frontend LoadBalancer IP not assigned yet (Attempt $((RETRY_COUNT+1))/$MAX_RETRIES). Waiting 30s..."
                          sleep 30
                        fi
                        RETRY_COUNT=$((RETRY_COUNT+1))
                      done

                      if [ -n "$FRONTEND_IP" ]; then
                        echo "✅ Frontend Endpoint (LoadBalancer): http://$FRONTEND_IP"
                      else
                        echo "⚠️ WARNING: Frontend LoadBalancer IP could not be retrieved after waiting."
                        echo "   Check 'kubectl get svc frontend-service -n $(k8sNamespace) -o wide' for status."
                      fi
                      echo ""

                      # Check API Services Cluster IPs
                      echo "Checking API Service Cluster IPs (Internal Access):"
                      for api_svc in auth-api-service users-api-service todos-api-service; do
                        CLUSTER_IP=$(kubectl get svc $api_svc -n $(k8sNamespace) -o jsonpath='{.spec.clusterIP}' 2>/dev/null)
                        PORT=$(kubectl get svc $api_svc -n $(k8sNamespace) -o jsonpath='{.spec.ports[0].port}' 2>/dev/null)
                        if [ -n "$CLUSTER_IP" ] && [ "$CLUSTER_IP" != "<none>" ]; then
                          echo "   - $api_svc: ClusterIP $CLUSTER_IP on port $PORT (Accessible within the cluster)"
                        else
                          echo "   - $api_svc: Could not retrieve ClusterIP or service not found/headless."
                        fi
                      done
                      echo ""
                      echo "=================================================="


                # STEP 8: Additional Debugging for Failing Pods (If any)
                - task: Bash@3
                  displayName: 'Debug Failing Pods (if any)'
                  condition: always()
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "==== Checking for non-Running Pods in namespace $(k8sNamespace) ===="
                      # Get names of pods that are NOT in Running state or whose containers are not Ready (e.g., 0/1 Running)
                      FAILING_PODS=$(kubectl get pods -n $(k8sNamespace) --field-selector=status.phase!=Running -o name)
                      READY_BUT_NOT_RUNNING=$(kubectl get pods -n $(k8sNamespace) --field-selector=status.phase=Running -o jsonpath='{range .items[?(@.status.containerStatuses[*].ready==false)]}{.metadata.name}{"\n"}{end}')

                      # Combine, remove empty lines, sort unique
                      ALL_ISSUES=$(echo "${FAILING_PODS}"$'\n'"${READY_BUT_NOT_RUNNING}" | grep . | sort -u)


                      if [ -z "$ALL_ISSUES" ]; then
                        echo "All pods appear to be Running and Ready."
                        exit 0 # Exit successfully
                      fi

                      echo "==== Describe and Log Pods with Issues ===="
                      # Use while read loop for safer iteration over names
                      echo "$ALL_ISSUES" | while IFS= read -r pod_resource_name; do
                        # Extract pod name if it's in format "pod/name"
                        pod_name=${pod_resource_name#pod/}
                        echo ""
                        echo "---- Describing $pod_name ----"
                        kubectl describe pod $pod_name -n $(k8sNamespace) || echo "Failed to describe $pod_name"
                        echo ""
                        echo "---- Current Logs for $pod_name ----"
                        kubectl logs $pod_name -n $(k8sNamespace) --all-containers=true --tail=100 || echo "Failed to get current logs for $pod_name"
                        echo ""
                        echo "---- Previous Logs for $pod_name (if restarted) ----"
                        kubectl logs $pod_name -n $(k8sNamespace) --all-containers=true --previous --tail=100 || echo "No previous logs or failed to get previous logs for $pod_name"
                        echo "-------------------------------------"
                        echo ""
                      done
                      # Exit with error if there were failing pods to make pipeline status reflect the issue
                      echo "Found pods with issues. See logs above."
                      exit 1