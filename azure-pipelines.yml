trigger:
  branches:
    include:
      - dev
      - master # Deploy main to prod, dev to dev environment
  paths:
    include: # Trigger only if app code or k8s manifests change
      - 'auth-api/'
      - 'users-api/'
      - 'todos-api/'
      - 'log-message-processor/'
      - 'frontend/'
      - 'k8s/'
      - 'azure-pipelines-app.yml'
    exclude:
      - '**/*.md' # Exclude markdown changes

pool:
  vmImage: ubuntu-latest

variables:
  # Determine environment based on branch
  - name: environment
    ${{ if eq(variables['Build.SourceBranchName'], 'master') }}:
      value: 'prod'
    ${{ if eq(variables['Build.SourceBranchName'], 'dev') }}:
      value: 'dev'
  # Link to the variable group containing infra outputs and secrets
  - group: app-variables-${{ variables.environment }} # e.g., app-variables-dev or app-variables-prod

  # Define image names consistently
  - name: imageAuthApi
    value: 'auth-api'
  - name: imageUsersApi
    value: 'users-api'
  - name: imageTodosApi
    value: 'todos-api'
  - name: imageLogProcessor
    value: 'log-message-processor'
  - name: imageFrontend
    value: 'frontend'

  # Kubernetes details
  - name: k8sNamespace
    value: 'apps-$(environment)'
  - name: k8sImagePullSecretName
    value: 'acr-secret'
  - name: k8sRedisSecretName
    value: 'redis-secret'
  - name: k8sAppSecretName
    value: 'app-secrets'
  - name: k8sManifestPath
    value: 'k8s'

  # Service Connection Names
  - name: acrServiceConnection
    value: 'ACR-Service-Connection' # Replace with your ACR Service Connection name
  - name: aksServiceConnection
    value: 'aks-prod-microAppProd-prod-pheasant-aks-apps-prod-1745099156012' # Replace with your AKS Service Connection name

stages:
  # =========================================================================
  # Stage 1: Build and Push ONLY CHANGED Docker Images
  # =========================================================================
  - stage: BuildAndPush
    displayName: Build & Push Changed Images
    jobs:
      - job: BuildPushMatrix
        displayName: Build and Push Services
        strategy:
          matrix:
            AuthApi:
              imageName: $(imageAuthApi)
              buildContext: 'auth-api'
            UsersApi:
              imageName: $(imageUsersApi)
              buildContext: 'users-api'
            TodosApi:
              imageName: $(imageTodosApi)
              buildContext: 'todos-api'
            LogProcessor:
              imageName: $(imageLogProcessor)
              buildContext: 'log-message-processor'
            Frontend:
              imageName: $(imageFrontend)
              buildContext: 'frontend'
        steps:
          # Step 1: Checkout with full history to allow diffing
          - checkout: self
            fetchDepth: 0 # Fetch all history for accurate diff
            displayName: Checkout Full History

          # Step 2: Detect changes within the service directory
          # NOTE: This simple diff compares HEAD to the previous commit (HEAD~1).
          # This works well for direct pushes to dev/master.
          # For more complex scenarios like PR builds merging into dev/master,
          # a more sophisticated script comparing against the target branch might be needed.
          - task: Bash@3
            name: DetectChanges # Give the step a name to reference its variables
            displayName: 'Detect Changes in $(buildContext)'
            inputs:
              targetType: 'inline'
              script: |
                set -e # Exit immediately if a command exits with a non-zero status.
                echo "Checking for changes in $(buildContext)/ between HEAD and HEAD~1"
                # Initialize ShouldBuild variable for this matrix instance
                echo "##vso[task.setvariable variable=ShouldBuild]false"

                # Use git diff --quiet. It exits with 1 if there are changes, 0 otherwise.
                # We check changes only within the specific buildContext directory.
                if git diff --quiet HEAD HEAD~1 -- "$(Build.SourcesDirectory)/$(buildContext)/"; then
                  echo "No changes detected in $(buildContext)."
                else
                  echo "Changes detected in $(buildContext). Setting ShouldBuild=true."
                  # Set the variable that the next step will check
                  echo "##vso[task.setvariable variable=ShouldBuild]true"
                fi

          # Step 3: Build and Push ONLY if changes were detected
          - task: Docker@2
            displayName: 'Build and Push $(imageName) (if changed)'
            # Condition: Only run if the ShouldBuild variable was set to true in the previous step
            condition: eq(variables['ShouldBuild'], 'true')
            inputs:
              containerRegistry: '$(acrServiceConnection)'
              repository: '$(imageName)'
              command: 'buildAndPush'
              Dockerfile: '$(Build.SourcesDirectory)/$(buildContext)/Dockerfile'
              buildContext: '$(Build.SourcesDirectory)/$(buildContext)'
              tags: '$(Build.BuildId)'
              addPipelineData: false

  # =========================================================================
  # Stage 2: Deploy to AKS
  # =========================================================================
  - stage: DeployToAKS
    displayName: Deploy Applications to AKS
    dependsOn: BuildAndPush
    # Condition: Run if Build stage succeeded OR was skipped (because nothing changed)
    # AND the branch is master or dev.
    condition: and(in(dependencies.BuildAndPush.result, 'Succeeded', 'SucceededWithIssues', 'Skipped'), or(eq(variables['Build.SourceBranchName'], 'master'), eq(variables['Build.SourceBranchName'], 'dev')))
    jobs:
      - deployment: DeployApps
        displayName: Deploy to AKS ($(environment))
        environment: 'aks-$(environment)'
        strategy:
          runOnce:
            deploy:
              steps:
                # STEP 0: Checkout the repository code
                - checkout: self
                  displayName: Checkout Repository

                # STEP 1: Debug - Verify k8s directory exists (with FI fix)
                - task: Bash@3
                  displayName: 'Debug: Check for K8s Manifests Directory'
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "Pipeline Workspace: $(Pipeline.Workspace)"
                      echo "Build Sources Directory: $(Build.SourcesDirectory)"
                      echo "Listing contents of $(Build.SourcesDirectory):"
                      ls -la $(Build.SourcesDirectory)
                      echo "Checking for $(Build.SourcesDirectory)/$(k8sManifestPath):"
                      if [ -d "$(Build.SourcesDirectory)/$(k8sManifestPath)" ]; then
                        echo "Directory $(Build.SourcesDirectory)/$(k8sManifestPath) found. Contents:"
                        ls -la "$(Build.SourcesDirectory)/$(k8sManifestPath)"
                      else
                        echo "ERROR: Directory $(Build.SourcesDirectory)/$(k8sManifestPath) NOT FOUND. Ensure the path is correct and repo was checked out."
                        exit 1
                      fi # <-- Fixed: Added fi

                # STEP 2: Replace Tokens in Manifests
                - task: replacetokens@5
                  displayName: 'Replace Tokens in K8s Manifests'
                  inputs:
                    rootDirectory: '$(Build.SourcesDirectory)/$(k8sManifestPath)'
                    targetFiles: '**/*.yaml'
                    encoding: 'auto'
                    writeBOM: false
                    actionOnMissing: 'warn'
                    keepToken: false
                    tokenPrefix: '#{'
                    tokenSuffix: '}#'
                    useLegacyPattern: false
                    variables: |
                      k8sNamespace = $(k8sNamespace)
                      acrName = $(acrName)
                      imageAuthApi = $(imageAuthApi)
                      imageUsersApi = $(imageUsersApi)
                      imageTodosApi = $(imageTodosApi)
                      imageLogProcessor = $(imageLogProcessor)
                      imageFrontend = $(imageFrontend)
                      Build.BuildId = $(Build.BuildId) # Image tag uses BuildId consistently
                      k8sImagePullSecretName = $(k8sImagePullSecretName)

                # STEP 3: Ensure Namespace exists
                - task: KubernetesManifest@0
                  displayName: Ensure Namespace $(k8sNamespace) exists
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    manifests: '$(Build.SourcesDirectory)/$(k8sManifestPath)/namespace.yaml'

                # # STEP 4: Create/Update Image Pull Secret (Consider removing if AKS-ACR integration is reliable)
                # - task: KubernetesManifest@0
                #   displayName: Create/Update Image Pull Secret ($(k8sImagePullSecretName))
                #   inputs:
                #     action: 'createSecret'
                #     kubernetesServiceConnection: '$(aksServiceConnection)'
                #     namespace: $(k8sNamespace)
                #     secretType: 'dockerRegistry'
                #     secretName: $(k8sImagePullSecretName)
                #     dockerRegistryEndpoint: '$(acrServiceConnection)'
                #     force: true

                # STEP 5: Create/Update Redis Secret
                - task: KubernetesManifest@0
                  displayName: Create/Update Redis Secret ($(k8sRedisSecretName))
                  inputs:
                    action: 'createSecret'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    secretType: 'generic'
                    secretName: $(k8sRedisSecretName)
                    secretArguments: '--from-literal=redisHost=$(redisHostName) --from-literal=redisKey=$(redisPrimaryKey)'
                    force: true

                # STEP 6: Create/Update App Secrets (JWT)
                - task: KubernetesManifest@0
                  displayName: Create/Update App Secrets (JWT) ($(k8sAppSecretName))
                  inputs:
                    action: 'createSecret'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    secretType: 'generic'
                    secretName: $(k8sAppSecretName)
                    secretArguments: '--from-literal=jwtSecret=$(jwtSecretValue)'
                    force: true

                # STEP 7: Apply App ConfigMap
                - task: KubernetesManifest@0
                  displayName: Apply App ConfigMap
                  condition: succeeded()
                  inputs:
                    action: 'deploy' # Changed to deploy for consistency, acts like apply
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: '$(Build.SourcesDirectory)/$(k8sManifestPath)/app-configmap.yaml'

                # STEP 8: Deploy Application Manifests
                - task: KubernetesManifest@0
                  displayName: Apply Application Manifests
                  inputs:
                    action: 'deploy' # Use deploy, it handles create/update
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: |
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/auth-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/auth-api-service.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/users-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/users-api-service.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/todos-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/todos-api-service.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/log-processor-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/frontend-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/frontend-service.yaml
                    # Image tag is now handled via token replacement in the YAML files

                # STEP 9: Verify Deployments and Output Endpoints
                - task: Bash@3
                  displayName: Verify Deployments & Output Endpoints
                  # Run even if previous deployment task has issues, to aid debugging
                  condition: always()
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "Waiting up to 5 minutes for deployments in namespace $(k8sNamespace) to stabilize..."
                      # Wait for all deployments in the target namespace to become available
                      if ! kubectl wait --namespace $(k8sNamespace) --for=condition=available deployment --all --timeout=5m; then
                        echo "ERROR: Deployments did not become available in time."
                        # Don't exit immediately, continue to gather debug info
                      else
                         echo "All deployments in $(k8sNamespace) are available."
                      fi

                      echo ""
                      echo "=================================================="
                      echo "Current Deployment Status"
                      echo "Namespace: $(k8sNamespace)"
                      echo "=================================================="
                      echo ""
                      echo "----- Deployment Status -----"
                      kubectl get deployment -n $(k8sNamespace)
                      echo ""
                      echo "----- Pod Status -----"
                      kubectl get pods -n $(k8sNamespace) -o wide
                      echo ""
                      echo "----- Services -----"
                      kubectl get svc -n $(k8sNamespace) -o wide
                      echo ""
                      echo "=================================================="
                      echo " Checking Service Endpoints..."
                      echo "=================================================="
                      echo ""

                      # Check Frontend LoadBalancer IP
                      echo "Checking Frontend LoadBalancer IP (Service: frontend-service)..."
                      FRONTEND_IP=""
                      RETRY_COUNT=0
                      MAX_RETRIES=6 # Wait up to 3 minutes (6 * 30s)
                      while [ -z "$FRONTEND_IP" ] && [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                        FRONTEND_IP=$(kubectl get svc frontend-service -n $(k8sNamespace) -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
                        if [ -z "$FRONTEND_IP" ]; then
                          echo "Frontend LoadBalancer IP not assigned yet (Attempt $((RETRY_COUNT+1))/$MAX_RETRIES). Waiting 30s..."
                          sleep 30
                        fi
                        RETRY_COUNT=$((RETRY_COUNT+1))
                      done

                      if [ -n "$FRONTEND_IP" ]; then
                        echo "✅ Frontend Endpoint (LoadBalancer): http://$FRONTEND_IP"
                      else
                        echo "⚠️ WARNING: Frontend LoadBalancer IP could not be retrieved after waiting."
                        echo "   Check 'kubectl get svc frontend-service -n $(k8sNamespace) -o wide' for status."
                      fi
                      echo ""

                      # Check API Services Cluster IPs
                      echo "Checking API Service Cluster IPs (Internal Access):"
                      for api_svc in auth-api-service users-api-service todos-api-service; do
                        CLUSTER_IP=$(kubectl get svc $api_svc -n $(k8sNamespace) -o jsonpath='{.spec.clusterIP}' 2>/dev/null)
                        PORT=$(kubectl get svc $api_svc -n $(k8sNamespace) -o jsonpath='{.spec.ports[0].port}' 2>/dev/null)
                        if [ -n "$CLUSTER_IP" ] && [ "$CLUSTER_IP" != "<none>" ]; then
                          echo "   - $api_svc: ClusterIP $CLUSTER_IP on port $PORT (Accessible within the cluster)"
                        else
                          echo "   - $api_svc: Could not retrieve ClusterIP or service not found/headless."
                        fi
                      done
                      echo ""
                      echo "=================================================="

                # STEP 10: Additional Debugging for Failing Pods (If any)
                - task: Bash@3
                  displayName: 'Debug Failing Pods (if any)'
                  # Run even if previous steps failed to capture logs/events
                  condition: always()
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "==== Checking for non-Running Pods in namespace $(k8sNamespace) ===="
                      # Get names of pods that are NOT in Running state or whose containers are not Ready (e.g., 0/1 Running)
                      FAILING_PODS=$(kubectl get pods -n $(k8sNamespace) --field-selector=status.phase!=Running -o name)
                      READY_BUT_NOT_RUNNING=$(kubectl get pods -n $(k8sNamespace) --field-selector=status.phase=Running -o jsonpath='{range .items[?(@.status.containerStatuses[*].ready==false)]}{.metadata.name}{"\n"}{end}')

                      ALL_ISSUES="${FAILING_PODS} ${READY_BUT_NOT_RUNNING}"

                      if [ -z "$ALL_ISSUES" ]; then
                        echo "All pods appear to be Running and Ready."
                        exit 0
                      fi

                      echo "==== Describe and Log Pods with Issues ===="
                      # Use xargs to handle potential multiple pod names correctly
                      echo "$ALL_ISSUES" | xargs -n 1 -I {} bash -c '
                        pod_name=$(basename {}) # Extract pod name from resource identifier like pod/my-pod-xxxx
                        echo ""
                        echo "---- Describing $pod_name ----"
                        kubectl describe pod $pod_name -n $(k8sNamespace) || echo "Failed to describe $pod_name"
                        echo ""
                        echo "---- Current Logs for $pod_name ----"
                        kubectl logs $pod_name -n $(k8sNamespace) --all-containers=true --tail=100 || echo "Failed to get current logs for $pod_name"
                        echo ""
                        echo "---- Previous Logs for $pod_name (if restarted) ----"
                        kubectl logs $pod_name -n $(k8sNamespace) --all-containers=true --previous --tail=100 || echo "No previous logs or failed to get previous logs for $pod_name"
                        echo "-------------------------------------"
                        echo ""
                      '
                      # Exit with error if there were failing pods to make pipeline status reflect the issue
                      echo "Found pods with issues. See logs above."
                      exit 1