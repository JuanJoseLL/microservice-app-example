trigger:
  branches:
    include:
      - dev
      - master
  paths:
    include:
      - 'auth-api/'
      - 'users-api/'
      - 'todos-api/'
      - 'log-message-processor/'
      - 'frontend/'
      - 'k8s/'
      - 'azure-pipelines-app.yml'
    exclude:
      - '**/*.md'

pool:
  vmImage: ubuntu-latest

variables:
  # ... (environment, group, image names, k8s details - same as before, remove k8sImagePullSecretName if not used) ...
  - name: k8sNamespace
    value: 'apps-$(environment)'
  - name: k8sRedisSecretName
    value: 'redis-secret'
  - name: k8sAppSecretName
    value: 'app-secrets'
  - name: k8sManifestPath
    value: 'k8s'
  - name: acrServiceConnection
    value: 'ACR-Service-Connection' # Replace
  - name: aksServiceConnection
    value: 'aks-prod-microAppProd-prod-pheasant-aks-apps-prod-1745099156012' # Replace

stages:
  # =========================================================================
  # Stage 1: Build and Push ONLY CHANGED Docker Images with Commit SHA Tag
  # =========================================================================
  - stage: BuildAndPush
    displayName: Build & Push Changed Images w/ Commit SHA
    jobs:
      - job: BuildPushMatrix
        displayName: Build and Push Services
        strategy:
          matrix:
            AuthApi: { imageName: $(imageAuthApi), buildContext: 'auth-api' }
            UsersApi: { imageName: $(imageUsersApi), buildContext: 'users-api' }
            TodosApi: { imageName: $(imageTodosApi), buildContext: 'todos-api' }
            LogProcessor: { imageName: $(imageLogProcessor), buildContext: 'log-message-processor' }
            Frontend: { imageName: $(imageFrontend), buildContext: 'frontend' }
        steps:
          - checkout: self
            fetchDepth: 0
            displayName: Checkout Full History

          - task: Bash@3
            name: DetectChanges
            displayName: 'Detect Changes in $(buildContext)'
            inputs:
              targetType: 'inline'
              script: |
                set -e
                SHOULD_BUILD="false"
                echo "Checking for changes in $(buildContext)/ between HEAD~1 and HEAD"
                if ! git diff --quiet HEAD~1 HEAD -- "$(Build.SourcesDirectory)/$(buildContext)/"; then
                  echo "Changes detected in $(buildContext). Setting SHOULD_BUILD=true."
                  SHOULD_BUILD="true"
                else
                  echo "No changes detected in $(buildContext)."
                fi
                echo "##vso[task.setvariable variable=ShouldBuild;isOutput=true]$SHOULD_BUILD"

          - task: Docker@2
            displayName: 'Build and Push $(imageName) with Commit SHA $(Build.SourceVersion) (if changed)'
            condition: eq(variables['DetectChanges.ShouldBuild'], 'true')
            inputs:
              containerRegistry: '$(acrServiceConnection)'
              repository: '$(imageName)'
              command: 'buildAndPush'
              Dockerfile: '$(Build.SourcesDirectory)/$(buildContext)/Dockerfile'
              buildContext: '$(Build.SourcesDirectory)/$(buildContext)'
              tags: '$(Build.SourceVersion)' # <-- Use Commit SHA for tag
              addPipelineData: false

  # =========================================================================
  # Stage 2: Deploy to AKS (Using Commit SHA Tag)
  # =========================================================================
  - stage: DeployToAKS
    displayName: Deploy Applications to AKS
    dependsOn: BuildAndPush
    # Run if build stage finished (even if skipped) and branch is right
    condition: and(in(dependencies.BuildAndPush.result, 'Succeeded', 'SucceededWithIssues', 'Skipped'), or(eq(variables['Build.SourceBranchName'], 'master'), eq(variables['Build.SourceBranchName'], 'dev')))
    jobs:
      - deployment: DeployApps
        displayName: Deploy to AKS ($(environment))
        environment: 'aks-$(environment)'
        strategy:
          runOnce:
            deploy:
              steps:
                # STEP 0: Checkout
                - checkout: self
                  displayName: Checkout Repository

                # STEP 1: Debug K8s Directory Check
                - task: Bash@3
                  displayName: 'Debug: Check for K8s Manifests Directory'
                  # ... (script as before) ...
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "Pipeline Workspace: $(Pipeline.Workspace)"
                      echo "Build Sources Directory: $(Build.SourcesDirectory)"
                      echo "Listing contents of $(Build.SourcesDirectory):"
                      ls -la $(Build.SourcesDirectory)
                      echo "Checking for $(Build.SourcesDirectory)/$(k8sManifestPath):"
                      if [ -d "$(Build.SourcesDirectory)/$(k8sManifestPath)" ]; then
                        echo "Directory $(Build.SourcesDirectory)/$(k8sManifestPath) found. Contents:"
                        ls -la "$(Build.SourcesDirectory)/$(k8sManifestPath)"
                      else
                        echo "ERROR: Directory $(Build.SourcesDirectory)/$(k8sManifestPath) NOT FOUND."
                        exit 1
                      fi

                # STEP 2: Replace Tokens (Using Commit SHA)
                - task: replacetokens@5
                  displayName: 'Replace Tokens in K8s Manifests'
                  inputs:
                    rootDirectory: '$(Build.SourcesDirectory)/$(k8sManifestPath)'
                    targetFiles: '**/*.yaml'
                    encoding: 'auto'
                    writeBOM: false
                    actionOnMissing: 'warn'
                    keepToken: false
                    tokenPrefix: '#{'
                    tokenSuffix: '}#'
                    useLegacyPattern: false
                    variables: |
                      k8sNamespace = $(k8sNamespace)
                      acrName = $(acrName)
                      commitSHA = $(Build.SourceVersion) # Use commit SHA

                # STEP 3: Ensure Namespace exists
                - task: KubernetesManifest@0
                  displayName: Ensure Namespace $(k8sNamespace) exists
                  # ... (inputs as before) ...
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    manifests: '$(Build.SourcesDirectory)/$(k8sManifestPath)/namespace.yaml'


                # STEP 4: Create/Update Secrets
                - task: KubernetesManifest@0
                  displayName: Create/Update Redis Secret ($(k8sRedisSecretName))
                  # ... (inputs as before) ...
                  inputs:
                    action: 'createSecret'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    secretType: 'generic'
                    secretName: $(k8sRedisSecretName)
                    secretArguments: '--from-literal=redisHost=$(redisHostName) --from-literal=redisKey=$(redisPrimaryKey)'
                    force: true

                - task: KubernetesManifest@0
                  displayName: Create/Update App Secrets (JWT) ($(k8sAppSecretName))
                  # ... (inputs as before) ...
                  inputs:
                    action: 'createSecret'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    secretType: 'generic'
                    secretName: $(k8sAppSecretName)
                    secretArguments: '--from-literal=jwtSecret=$(jwtSecretValue)'
                    force: true

                # STEP 5: Apply App ConfigMap
                - task: KubernetesManifest@0
                  displayName: Apply App ConfigMap
                  # ... (inputs as before) ...
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    manifests: '$(Build.SourcesDirectory)/$(k8sManifestPath)/app-configmap.yaml'


                # --- STEP 6: Apply ALL Application Manifests ---
                # No conditions needed now, kubectl apply handles idempotency
                - task: KubernetesManifest@0
                  displayName: Apply Application Manifests
                  inputs:
                    action: 'deploy'
                    kubernetesServiceConnection: '$(aksServiceConnection)'
                    namespace: $(k8sNamespace)
                    # Apply all relevant manifests from the k8s directory
                    # Ensure your YAML files use #{commitSHA}# for the image tag
                    manifests: |
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/auth-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/auth-api-service.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/users-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/users-api-service.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/todos-api-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/todos-api-service.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/log-processor-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/frontend-deployment.yaml
                      $(Build.SourcesDirectory)/$(k8sManifestPath)/frontend-service.yaml

                # STEP 7: Verify Deployments & Output Endpoints
                - task: Bash@3
                  displayName: Verify Deployments & Output Endpoints
                  condition: always()
                  # ... (script remains the same) ...
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "Waiting up to 5 minutes for deployments in namespace $(k8sNamespace) to stabilize..."
                      if ! kubectl wait --namespace $(k8sNamespace) --for=condition=available deployment --all --timeout=5m; then
                        echo "WARNING: Not all deployments became available/stable within the timeout."
                      else
                         echo "All relevant deployments in $(k8sNamespace) appear available/stable."
                      fi
                      echo ""
                      echo "=================================================="
                      echo "Current Deployment Status"
                      echo "Namespace: $(k8sNamespace)"
                      echo "=================================================="
                      echo ""
                      echo "----- Deployment Status -----"
                      kubectl get deployment -n $(k8sNamespace)
                      echo ""
                      echo "----- Pod Status -----"
                      kubectl get pods -n $(k8sNamespace) -o wide
                      echo ""
                      echo "----- Services -----"
                      kubectl get svc -n $(k8sNamespace) -o wide
                      echo ""
                      echo "=================================================="
                      echo " Checking Service Endpoints..."
                      echo "=================================================="
                      echo ""
                      echo "Checking Frontend LoadBalancer IP (Service: frontend-service)..."
                      FRONTEND_IP=""
                      RETRY_COUNT=0
                      MAX_RETRIES=6
                      while [ -z "$FRONTEND_IP" ] && [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                        FRONTEND_IP=$(kubectl get svc frontend-service -n $(k8sNamespace) -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
                        if [ -z "$FRONTEND_IP" ]; then
                          echo "Frontend LoadBalancer IP not assigned yet (Attempt $((RETRY_COUNT+1))/$MAX_RETRIES). Waiting 30s..."
                          sleep 30
                        fi
                        RETRY_COUNT=$((RETRY_COUNT+1))
                      done
                      if [ -n "$FRONTEND_IP" ]; then
                        echo "✅ Frontend Endpoint (LoadBalancer): http://$FRONTEND_IP"
                      else
                        echo "⚠️ WARNING: Frontend LoadBalancer IP could not be retrieved after waiting."
                        echo "   Check 'kubectl get svc frontend-service -n $(k8sNamespace) -o wide' for status."
                      fi
                      echo ""
                      echo "Checking API Service Cluster IPs (Internal Access):"
                      for api_svc in auth-api-service users-api-service todos-api-service; do
                        CLUSTER_IP=$(kubectl get svc $api_svc -n $(k8sNamespace) -o jsonpath='{.spec.clusterIP}' 2>/dev/null)
                        PORT=$(kubectl get svc $api_svc -n $(k8sNamespace) -o jsonpath='{.spec.ports[0].port}' 2>/dev/null)
                        if [ -n "$CLUSTER_IP" ] && [ "$CLUSTER_IP" != "<none>" ]; then
                          echo "   - $api_svc: ClusterIP $CLUSTER_IP on port $PORT (Accessible within the cluster)"
                        else
                          echo "   - $api_svc: Could not retrieve ClusterIP or service not found/headless."
                        fi
                      done
                      echo ""
                      echo "=================================================="

                # STEP 8: Additional Debugging for Failing Pods (If any)
                - task: Bash@3
                  displayName: 'Debug Failing Pods (if any)'
                  condition: always()
                  # ... (script remains the same) ...
                  inputs:
                    targetType: 'inline'
                    script: |
                      echo "==== Checking for non-Running Pods in namespace $(k8sNamespace) ===="
                      FAILING_PODS=$(kubectl get pods -n $(k8sNamespace) --field-selector=status.phase!=Running -o name)
                      READY_BUT_NOT_RUNNING=$(kubectl get pods -n $(k8sNamespace) --field-selector=status.phase=Running -o jsonpath='{range .items[?(@.status.containerStatuses[*].ready==false)]}{.metadata.name}{"\n"}{end}')
                      ALL_ISSUES=$(echo "${FAILING_PODS}"$'\n'"${READY_BUT_NOT_RUNNING}" | grep . | sort -u)
                      if [ -z "$ALL_ISSUES" ]; then
                        echo "All pods appear to be Running and Ready."
                        exit 0
                      fi
                      echo "==== Describe and Log Pods with Issues ===="
                      echo "$ALL_ISSUES" | while IFS= read -r pod_resource_name; do
                        pod_name=${pod_resource_name#pod/}
                        echo ""
                        echo "---- Describing $pod_name ----"
                        kubectl describe pod $pod_name -n $(k8sNamespace) || echo "Failed to describe $pod_name"
                        echo ""
                        echo "---- Current Logs for $pod_name ----"
                        kubectl logs $pod_name -n $(k8sNamespace) --all-containers=true --tail=100 || echo "Failed to get current logs for $pod_name"
                        echo ""
                        echo "---- Previous Logs for $pod_name (if restarted) ----"
                        kubectl logs $pod_name -n $(k8sNamespace) --all-containers=true --previous --tail=100 || echo "No previous logs or failed to get previous logs for $pod_name"
                        echo "-------------------------------------"
                        echo ""
                      done
                      echo "Found pods with issues. See logs above."
                      exit 1